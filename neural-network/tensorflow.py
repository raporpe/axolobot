# -*- coding: utf-8 -*-
"""twitter sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ovb3menwcGIgWQosYOEXjivdOLCQaWsX
"""

from tensorflow.keras import layers
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from collections import Counter
from nltk.corpus import stopwords
import nltk
import string
import re
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import time


df = pd.read_csv("./train.csv")

df.shape

df.head()

print((df.label == 1).sum())  # Positive
print((df.label == 0).sum())  # Negative


def remove_URL(text):
    url = re.compile(r"https?://\S+|www\.\S+")
    return url.sub(r"", text)


def remove_punct(text):
    translator = str.maketrans("", "", string.punctuation)
    return text.translate(translator)


string.punctuation

df["tweet"] = df.tweet.map(remove_URL)
df["tweet"] = df.tweet.map(remove_punct)

nltk.download('stopwords')

stop = set(stopwords.words("english"))


def remove_stopwords(text):
    filtered_words = [word.lower()
                      for word in text.split() if word.lower() not in stop]
    return " ".join(filtered_words)


stop

df["tweet"] = df.tweet.map(remove_stopwords)


def counter_word(text_col):
    count = Counter()
    for text in text_col.values:
        for word in text.split():
            count[word] += 1
    return count


counter = counter_word(df.tweet)

len(counter)

counter  # palabras con la cnatidad de apariciones

counter.most_common(5)

num_unique_words = len(counter)

# split data into training and validation

train_size = int(df.shape[0]*0.8)

train_df = df[:train_size]
val_df = df[train_size:]

train_sentences = train_df.tweet.to_numpy()
train_labels = train_df.label.to_numpy()
val_sentences = val_df.tweet.to_numpy()
val_labels = val_df.label.to_numpy()

train_sentences.shape, val_sentences.shape


# vectorize by turning each text into a sequence of integers
tokenizer = Tokenizer(num_words=num_unique_words)
tokenizer.fit_on_texts(train_sentences)  # fit only to training

# each word has unique index
word_index = tokenizer.word_index

word_index

train_sequences = tokenizer.texts_to_sequences(train_sentences)
val_sequences = tokenizer.texts_to_sequences(val_sentences)

print(train_sentences[10:15])
print(train_sequences[10:15])

# pad sequences to have the same lenght

# max number of words in a sequence
max_length = 20

train_padded = pad_sequences(
    train_sequences, maxlen=max_length, padding="post", truncating="post")
val_padded = pad_sequences(
    val_sequences, maxlen=max_length, padding="post", truncating="post")

train_padded.shape, val_padded.shape

train_padded[10]

print(train_sentences[10])
print(train_sequences[10])
print(train_padded[10])

# check reversing the indices
# flip(key,value)

reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])

reverse_word_index


def decode(sequence):
    return " ".join([reverse_word_index.get(idx, "?") for idx in sequence])


decoded_text = decode(train_sequences[10])

print(train_sequences[10])
print(decoded_text)


# Word embeddings give us a way to use an efficient, dense representation in which similar words have
# a similar enconding. Importantly, you do not have to especify this encoding by hand.
# Am embeding is a dense vector of floating point values (lenght of vector is a parameter)

model = keras.models.Sequential()
model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))

# The layer will take as input an integer matrix of size(batch,input_lenght),
# and the largest integer (i.e word index) in the input should be no larger than num_words (vocabulary size).
# Now model.output_shape is (None, nput_lenght, 32), where 'None' is the batch dimension

model.add(layers.LSTM(64, dropout=0.1))
model.add(layers.Dense(1, activation="sigmoid"))

model.summary()

loss = keras.losses.BinaryCrossentropy(from_logits=False)
optim = keras.optimizers.Adam(lr=0.001)
metrics = ["accuracy"]

model.compile(loss=loss, optimizer=optim, metrics=metrics)

model.fit(train_padded, train_labels, epochs=20,
          validation_data=(val_padded, val_labels), verbose=2)

predictions = model.predict(train_padded)
predictions = ["positive" if p > 0.7 else "neutral" if p >
               0.5 else "negative" for p in predictions]

print(train_sentences[10:20])
print(train_labels[10:20])
print(predictions[10:20])

val_predictions = model.predict(val_padded)
val_predictions = ["positive" if p > 0.7 else "neutral" if p >
                   0.5 else "negative" for p in val_predictions]
print(val_sentences[50:55])
print(val_labels[50:55])
print(val_predictions[50:55])

"""Para cuando use datasets con varias clases:

En vez de funcion sigmoide y 1 neurona de salida tendre que usar funcion softmax con el mismo numero de neuronas que clases.

Creo que para que esto funcione las clases que le metes a model.fit deben tener la dimension (batch,num_clases) por lo que si tenemos 3 clases una instancia que sea de la clase 1 su 'y' sera [1,0,0]

A este proceso se le llama one hot enconding y tensorflow trae una funci√≥n para realizarlo automaticamente

import tensorflow as tf

category_indices = [0, 1, 2, 2, 1, 0]

unique_category_count = 3

inputs = tf.one_hot(category_indices, unique_category_count)

print(inputs.numpy())

[[1. 0. 0.] <- category '0'

[0. 1. 0.] <- category '1'

[0. 0. 1.] <- category '2'

"""
